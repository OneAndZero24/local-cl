# @package _global_
# Interval Protection Unlearning on MNIST with MLP
# Fast test configuration with smaller model and fewer epochs

defaults:
  - dataset: mnist_pytorch
  - model: mlp_unlearn
  - _self_

exp:
  run_func:
    _target_: src.unlearn.unlearn_experiment
  
  seed: 42
  
  # Unlearning specific parameters
  unlearn_classes:
    - 0
    - 1
  
  # Interval protection ENABLED
  use_interval_protection: true
  lambda_interval: 100.0  # Weight for interval protection loss
  
  # Pretraining parameters
  pretrain_epochs: 5
  pretrain_lr: 0.001
  
  # Unlearning parameters
  batch_size: 128
  epochs: 5
  lr: 0.0001
  criterion: CrossEntropyLoss
  
  # Logging
  detect_anomaly: False
  log_dir: ${hydra:runtime.output_dir}
  cleanup: False

fabric:
  _target_: lightning.Fabric
  num_nodes: 1
  devices: 1
  accelerator: cpu

wandb:
  entity: ${oc.env:WANDB_ENTITY}
  project: unlearning-experiments
  name: mnist_mlp_interval_protection
  mode: online
