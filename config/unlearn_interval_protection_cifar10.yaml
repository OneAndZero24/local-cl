# @package _global_
# Interval Protection Unlearning: Negative loss + interval protection
# Protects the "negative space" where retain classes are active but unlearn classes are not
# This should preserve knowledge of retain classes better than baseline

defaults:
  - dataset: cifar10_pytorch
  - model: resnet18_unlearn
  - _self_

exp:
  run_func:
    _target_: src.unlearn.unlearn_experiment
  
  seed: 42
  
  # Unlearning specific parameters
  unlearn_classes:
    - 0
    - 1
  
  # Interval protection ENABLED
  use_interval_protection: true
  lambda_interval: 100.0  # Weight for interval protection loss
  margin_percentile: 0.2  # Margin to shrink forget interval (0.2 = 20% on each side)
  infinity_scale: 100.0  # Scale factor for infinity bounds (default: 10.0)
  
  # Pretraining parameters (if model_path is null)
  pretrain_epochs: 10
  pretrain_lr: 0.001
  
  # Training parameters
  batch_size: 128
  epochs: 10
  lr: 0.0001
  criterion: CrossEntropyLoss
  
  # Logging
  detect_anomaly: False
  log_dir: ${hydra:runtime.output_dir}
  cleanup: False

fabric:
  _target_: lightning.Fabric
  num_nodes: 1
  devices: 1
  accelerator: cpu # or gpu

wandb:
  entity: ${oc.env:WANDB_ENTITY}
  project: unlearning-experiments
  name: interval_protection_cifar10_unlearn_0_1
  mode: online
