_target_: model.MLP
initial_out_features: 1
sizes: [1, 512, 256, 128]  # Deeper network for better fit
layers: ["Normal", "Normal", "Normal"]
head_type: Regression
activation: GELU  # Better than ReLU, no dead neurons
mask_past_classifier_neurons: False
config:
  activation: 'none'  # No activation on output - linear regression head
